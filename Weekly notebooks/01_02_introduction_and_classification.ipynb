{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 and 2: Introduction and Data-driven Image Classification\n",
    "\n",
    "Welcome to this first hands-on tutorial! In this notebook, we’ll explore how to train a neural network for image classification using **PyTorch**, a popular deep learning framework. Our dataset of choice is the **CIFAR-10 dataset**, a well-known collection of 60,000 color images (32x32 pixels each) across 10 categories, such as airplanes, cats, and cars. Each category contains 6,000 images, providing a rich dataset for learning.\n",
    "\n",
    "For this task, we will build a **Multi-Layer Perceptron (MLP)**, a foundational type of artificial neural network. An MLP consists of:\n",
    "\n",
    "- **Input Layer**: Processes the raw image data.\n",
    "- **Hidden Layers**: Extract meaningful patterns and features.\n",
    "- **Output Layer**: Maps the learned features to one of the 10 image classes.\n",
    "\n",
    "The connections between nodes in these layers are weighted, and the network learns to adjust these weights during training to improve classification accuracy.\n",
    "\n",
    "By the end of this notebook, you’ll understand how to:\n",
    "1. Preprocess image data for neural network training.\n",
    "2. Build and train an MLP using PyTorch.\n",
    "3. Evaluate the performance of the trained model.\n",
    "\n",
    "Let’s dive in and start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries and load the CIFAR-10 dataset\n",
    "\n",
    "In this step, we’ll prepare our tools and data to kickstart the project. First, we’ll import the necessary libraries that make it possible to handle data, build and train neural networks, and visualize results effectively. \n",
    "\n",
    "Next, we’ll load the **CIFAR-10 dataset** using `torchvision`. As a quick refresher, the CIFAR-10 dataset contains 60,000 color images (32x32 pixels) evenly distributed across 10 classes (e.g., airplanes, cats, and cars). Each class has 6,000 images, providing a balanced and diverse dataset for training.\n",
    "\n",
    "To ensure optimal model performance, we’ll also **normalize** the images. Normalization scales the pixel values of the images to a similar range, typically around 0, making it easier for the model to learn and speeding up the convergence process during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_dataset = CIFAR10(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=False, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s take a moment to **visualize some images from our dataset**. This is a crucial step to better understand the data we’ll be working with. By examining these images, we can gain insights into the patterns, textures, and challenges involved in classifying them correctly. This also helps us appreciate the complexity of the task our model will tackle as it learns to distinguish between the 10 diverse classes in CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a 'cuda' device type for generator but found 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# get some random training images\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m train_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_iterator)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# show images\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1199\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1197\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1236\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[1;34m(self, loader, first_iter)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[1;32m-> 1236\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1486\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:698\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\sampler.py:344\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m [\u001b[38;5;241m*\u001b[39mbatch_droplast]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mitertools\u001b[38;5;241m.\u001b[39mislice(sampler_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m batch:\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\sampler.py:198\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[\n\u001b[0;32m    200\u001b[0m         : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n\n\u001b[0;32m    201\u001b[0m     ]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected a 'cuda' device type for generator but found 'cpu'"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # undo normalization\n",
    "    img = img.numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "train_iterator = iter(train_dataloader)\n",
    "images, labels = next(train_iterator)\n",
    "\n",
    "# show images\n",
    "imshow(make_grid(images, nrow=10))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define a simple Multi-Layer Perceptron (MLP)\n",
    "\n",
    "Next, we’ll design a simple **Multi-Layer Perceptron (MLP)** to perform image classification on the CIFAR-10 dataset. An MLP is a foundational type of artificial neural network that uses multiple layers to learn patterns in data.\n",
    "\n",
    "An MLP consists of:\n",
    "- **Input Layer**: Receives the raw input data (in our case, (normalized) image pixels).\n",
    "- **Hidden Layers**: Learn meaningful features by transforming input data through weighted connections and activation functions.\n",
    "- **Output Layer**: Maps the learned features to specific classes (10 in this case).\n",
    "\n",
    "Each node (or neuron) in one layer is connected to every node in the next layer through a weighted connection, allowing the network to capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # TODO: Define your own layers\n",
    "        self.fc1 = nn.Linear(32 * 32 * 3, 120)  # replace with your implementation (make sure the input layer is compatible with the CIFAR-10 images with a size of 32*32*3)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # replace with your implementation (make sure the output layer is compatible with the number of classes (n=10) in CIFAR-10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32*32*3)  # flatten the images\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = MLP()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a loss function and optimizer  \n",
    "\n",
    "In this step, we’ll define two key components of our model training process: the **loss function** and the **optimizer**.  \n",
    "\n",
    "- The **loss function** quantifies the difference between the model's predictions and the true labels. It provides feedback that guides the model on how to improve its predictions. For our classification task, we’ll use **Cross Entropy Loss**, which is well-suited for multi-class problems like CIFAR-10.  \n",
    "\n",
    "- The **optimizer** is responsible for adjusting the model’s parameters (weights and biases) to minimize the loss function. We’ll use **Stochastic Gradient Descent (SGD)**, a widely-used optimization algorithm that updates parameters incrementally during training to improve performance.  \n",
    "\n",
    "Together, these tools will enable our model to learn effectively from the data and improve its accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define your own loss function\n",
    "def my_loss_function(outputs, labels):\n",
    "    # Implement your own version of CrossEntropyLoss\n",
    "    # Hint: CrossEntropyLoss is a combination of LogSoftmax and NLLLoss\n",
    "    # You might want to use torch.log and torch.exp functions\n",
    "\n",
    "    # Calculate softmax of the outputs\n",
    "    softmax_outputs = torch.exp(outputs) / torch.sum(torch.exp(outputs), dim=1, keepdim=True)\n",
    "    # Calculate negative log likelihood\n",
    "    neg_log_likelihood = -torch.log(softmax_outputs[range(labels.shape[0]), labels])\n",
    "    print(neg_log_likelihood)\n",
    "\n",
    "    # TODO: Complete the implementation by averaging the negative log likelihood across the batch to get the final loss\n",
    "    avg_neg_log_likelihood = torch.sum(neg_log_likelihood) / len(outputs)\n",
    "    print(labels.shape[0])\n",
    "\n",
    "    return avg_neg_log_likelihood\n",
    "\n",
    "\n",
    "criterion = my_loss_function\n",
    "optimizer = SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the network and visualize the loss  \n",
    "\n",
    "Now, it’s time to train our neural network. **Training** involves optimizing the model’s parameters (weights and biases) to minimize the loss function, allowing the network to make better predictions over time.  \n",
    "\n",
    "The training process consists of the following key steps:  \n",
    "1. **Forward Pass**: The training data is passed through the network, and predictions are generated.  \n",
    "2. **Loss Calculation**: The difference between the model’s predictions and the true labels is measured using the loss function.  \n",
    "3. **Backward Pass**: Gradients of the loss with respect to each parameter are calculated through backpropagation, allowing the model to learn how to adjust its parameters.  \n",
    "4. **Parameter Update**: The optimizer updates the model’s parameters in the direction that reduces the loss, using the gradients calculated in the backward pass.  \n",
    "\n",
    "We’ll repeat this process for several **epochs** (complete passes through the training data). After each epoch, we’ll visualize the loss to track how well the network is learning. A decreasing loss indicates that the model is improving its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a 'cuda' device type for generator but found 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m):  \u001b[38;5;66;03m# loop over the dataset multiple times\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# get the inputs; data is a list of [inputs, labels]\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# zero the parameter gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1199\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1197\u001b[0m _utils\u001b[38;5;241m.\u001b[39msignal_handling\u001b[38;5;241m.\u001b[39m_set_SIGCHLD_handler()\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker_pids_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1236\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._reset\u001b[1;34m(self, loader, first_iter)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers):\n\u001b[1;32m-> 1236\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1486\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_put_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1483\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefetch_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1486\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:698\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter._next_index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\sampler.py:344\u001b[0m, in \u001b[0;36mBatchSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m [\u001b[38;5;241m*\u001b[39mbatch_droplast]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mitertools\u001b[38;5;241m.\u001b[39mislice(sampler_iter, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)]\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m batch:\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\env_5LSM0\\lib\\site-packages\\torch\\utils\\data\\sampler.py:198\u001b[0m, in \u001b[0;36mRandomSampler.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n):\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrandperm(n, generator\u001b[38;5;241m=\u001b[39mgenerator)\u001b[38;5;241m.\u001b[39mtolist()[\n\u001b[0;32m    200\u001b[0m         : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m%\u001b[39m n\n\u001b[0;32m    201\u001b[0m     ]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected a 'cuda' device type for generator but found 'cpu'"
     ]
    }
   ],
   "source": [
    "loss_values = []  # to store loss values\n",
    "for epoch in range(40):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        print(net)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # add the loss to running_loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # calculate average loss for this epoch\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print('Epoch [%d] loss: %.3f' % (epoch + 1, avg_loss))\n",
    "    loss_values.append(avg_loss)  # store the loss value\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training Loss per Epoch\")\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel(\"batches\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Test the network on test data and visualize the results  \n",
    "\n",
    "Finally, we’ll evaluate our trained network on the test dataset to see how well it performs on data it hasn’t encountered during training. Testing involves feeding the test data through the network and observing how accurately it classifies the images.  \n",
    "\n",
    "We’ll also visualize some test images alongside the model’s predictions. This simple but powerful step helps us understand the network’s strengths and weaknesses by identifying patterns it correctly classifies and areas where it might struggle.  \n",
    "\n",
    "By observing the predictions, we can gain valuable insights into the network’s performance and how well it has learned the CIFAR-10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_iterator = iter(test_dataloader)\n",
    "images, labels = next(test_iterator)\n",
    "\n",
    "# print images\n",
    "imshow(make_grid(images, nrow=10))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(10)))\n",
    "\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
